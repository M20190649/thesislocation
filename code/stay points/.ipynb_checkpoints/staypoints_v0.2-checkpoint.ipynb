{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "No data to be processed. Change user or date range and try again.\n"
     ]
    }
   ],
   "source": [
    "#Finding stay points from a user trajectory data and calculate transition matrices.\n",
    "#Stay points are goverened by two parameters, time threshold and distance threshold.\n",
    "#If user has spend more than threshold time(10 mins) at the same geographical location(threshold distance 50m),\n",
    "# then this location is users stay point.\n",
    "#This can be users home, work, a bus station or a restaurant. \n",
    "%reset\n",
    "\n",
    "#load all the files for a user\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import math \n",
    "import os\n",
    "import errno\n",
    "import matplotlib.patches as patches\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Ellipse\n",
    "import operator\n",
    "import pdb\n",
    "#%matplotlib nbagg\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#-----------------------------------LOAD USER FILES-----------------------------------------------------\n",
    "def load_user_file():\n",
    "    global combined_df\n",
    "    #Load file names for user. Change here for different user replace 000 or even path as required\n",
    "    filenames = glob.glob(file_source_raw)\n",
    "\n",
    "    #Read the files\n",
    "    list_of_dfs = [pd.read_csv(filename, skiprows=6, header = None) for filename in filenames]\n",
    "\n",
    "    #put the data from list into one dataframe\n",
    "    combined_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "    #rename columns\n",
    "    combined_df.columns = ['Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "    combined_df['ClusterId'] = -1\n",
    "    combined_df['ClusterMeanLat'] = -1\n",
    "    combined_df['ClusterMeanLon'] = -1\n",
    "    combined_df['StayPoint'] = -1\n",
    "    combined_df['SignificantPlace'] = -1\n",
    "\n",
    "    #add timestamp index\n",
    "    combined_df[\"Timestamp\"] = combined_df[\"Date\"].map(str) + \" \" + combined_df[\"Time\"]\n",
    "    combined_df.Timestamp = pd.to_datetime(combined_df.Timestamp)\n",
    "    combined_df.index = pd.to_datetime(combined_df.Timestamp)\n",
    "    \n",
    "    #add weekday number as column\n",
    "    combined_df['Weekday'] = combined_df['Timestamp'].dt.weekday.map(str) + combined_df['Timestamp'].dt.weekday_name\n",
    "\n",
    "    #combined_df.size\n",
    "    #combined_df.head()\n",
    "\n",
    "#-----------------------------------SAMPLE DATA PER MINUTE-----------------------------------------------------\n",
    "\n",
    "def resample_select_data():\n",
    "    global combined_df\n",
    "    global sampled_df\n",
    "    #Resample the data with every one minutes. Remove this if you like to process the entire file.\n",
    "    #Note, it could take some time to run the further sections of the file size is very large.\n",
    "    sampled_df = combined_df.resample('1T').mean()\n",
    "    sampled_df[\"Timestamp\"] = sampled_df.index\n",
    "    sampled_df = sampled_df.dropna()\n",
    "    \n",
    "    #Select the range of data you want to proceed with\n",
    "    if fltr_data_date_rng == \"YES\":\n",
    "        sampled_df = sampled_df[(sampled_df[\"Timestamp\"] >= from_date) & (sampled_df[\"Timestamp\"] <= to_date)]\n",
    "\n",
    "#-----------------------------------VISUALIZE RAW DATA--------------------------------------------------------\n",
    "\n",
    "def visualize_raw_data():\n",
    "    \n",
    "    if fltr_data_date_rng == \"YES\":\n",
    "        combined_c_df = combined_df[(combined_df[\"Timestamp\"] >= from_date) & (combined_df[\"Timestamp\"] <= to_date)]\n",
    "    else:\n",
    "        combined_c_df = combined_df\n",
    "        \n",
    "    #Plot 1------------------\n",
    "    #plot hourly vs weekly data of the user\n",
    "    #Resample data on hourly basis\n",
    "    hour_sampled_df = combined_c_df.resample('H').last()\n",
    "\n",
    "    #drop column timestamp to avoid confusion as index is also timestamp\n",
    "    hour_sampled_df = hour_sampled_df.drop(['Timestamp'], axis=1)\n",
    "\n",
    "    #extract time from timestamp and add it as a column\n",
    "    hour_sampled_df['TimeSampled'] = hour_sampled_df.index.time\n",
    "\n",
    "    #form a pivot table counting number of latitudes for each weekday for each hour\n",
    "    pivot_df = hour_sampled_df.pivot_table(values='Latitude', index='Weekday',columns='TimeSampled',aggfunc=len)\n",
    "\n",
    "    #draw a plot to visualize hourly trend per weekday for the user\n",
    "    fig, ax = plt.subplots(figsize=(20,5))  \n",
    "    sns.heatmap(pivot_df, cmap='BuGn', ax=ax)\n",
    "    plt.title('Weekly - Hourly Plot')\n",
    "    plt.show()\n",
    "    \n",
    "    #Plot 2------------------\n",
    "    #visualize counts of data for each month-year\n",
    "    count_df = pd.DataFrame()\n",
    "    count_df['count'] = combined_c_df.groupby(combined_c_df.index.year * 100 + combined_c_df.index.month).size() \n",
    "    count_df.plot(kind='bar', title =\"V comp\",figsize=(5,5),legend=True, fontsize=12, color = 'b')\n",
    "    plt.title('MonthYear Distribution Plot')\n",
    "    plt.show()\n",
    "    \n",
    "    #Plot 3------------------\n",
    "    #plot to visualize the location trend of the user\n",
    "    #Sample data hourly and take mean, drop nans, reset index\n",
    "    xy_df = combined_c_df.resample('H').mean()\n",
    "    xy_df = xy_df.dropna()\n",
    "    xy_df['Timestamp'] = xy_df.index\n",
    "    xy_df = xy_df.reset_index(drop=True)\n",
    "\n",
    "    #assign the first lat and log as the base for the plot i.e. origin\n",
    "    origin_lat = math.radians(xy_df[\"Latitude\"][0])\n",
    "    origin_lon = math.radians(xy_df[\"Longitude\"][0])\n",
    "    \n",
    "    #convert each lat and lon into x and y for the plot w.r.t origin\n",
    "    EARTH_RAD = 6378100\n",
    "    xy_df['X'] = 0.0\n",
    "    xy_df['Y'] = 0.0\n",
    "    for i in range(0, len(xy_df)):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        current_lat = math.radians(xy_df[\"Latitude\"][i])\n",
    "        current_lon = math.radians(xy_df[\"Longitude\"][i])\n",
    "        x = ((math.cos(current_lat) + math.cos(origin_lat))/2) * EARTH_RAD * (current_lon - origin_lon) * math.pi / 180\n",
    "        y = (current_lat - origin_lat)* math.pi/180 * EARTH_RAD\n",
    "        xy_df.at[i, 'X'] = x\n",
    "        xy_df.at[i, 'Y'] = y\n",
    "    \n",
    "    xy_copy_df = xy_df[['X', 'Y', 'Timestamp']].copy()\n",
    "    xy_copy_df['Hour'] = xy_copy_df.Timestamp.dt.hour\n",
    "    del xy_copy_df['Timestamp']\n",
    "    fg = sns.FacetGrid(data=xy_copy_df, hue='Hour',  size = 10, aspect = 2)\n",
    "    fg.map(plt.scatter, 'X', 'Y', edgecolor=\"w\", s=400).add_legend()\n",
    "    plt.title('User Movement Trend Plot')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    xticks = np.arange(-1000,1000,50)\n",
    "    yticks = np.arange(-1000,1000,50)\n",
    "    fg.set(xticks=xticks, yticks=yticks)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "#-----------------------------------METER DISTANCE BETWEEN TWO COORDINATES---------------------------------------\n",
    "\n",
    "#Find distance between two lan:lon points in meters\n",
    "def meters(lat1, lon1, lat2, lon2):  \n",
    "    R = 6378.137 # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat/2) * math.sin(dLat/2) + math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * math.sin(dLon/2) * math.sin(dLon/2);\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "    d = R * c\n",
    "    return d * 1000 # meters\n",
    "\n",
    "#This function is to cluster the points together if thier distance is less than 50 meters.\n",
    "#If the cluster has a total duration of 10 minutes or greater, then add it to a stay point\n",
    "\n",
    "#-----------------------------------CLUSTER POINTS----------------------------------------------------------\n",
    "\n",
    "def cluster(newlat, newlon, row, count):\n",
    "    global sampled_df\n",
    "    \n",
    "    currcluster = sampled_df['ClusterId'][row-1]\n",
    "    sampled_df['ClusterId'][row] = -1\n",
    "    sampled_df['ClusterMeanLat'][row] = -1\n",
    "    sampled_df['ClusterMeanLon'][row] = -1\n",
    "    sampled_df['StayPoint'][row] = -1\n",
    "    sampled_df['SignificantPlace'][row] = -1\n",
    "    clulat = sampled_df['ClusterMeanLat'][row-1]\n",
    "    clulon = sampled_df['ClusterMeanLon'][row-1]\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    if meters(clulat, clulon, newlat, newlon)<= 100:\n",
    "        sampled_df['ClusterId'][row] = currcluster\n",
    "        sampled_df['ClusterMeanLat'] = sampled_df.groupby('ClusterId')['Latitude'].transform(np.mean)\n",
    "        sampled_df['ClusterMeanLon'] = sampled_df.groupby('ClusterId')['Longitude'].transform(np.mean)\n",
    "        count = count + 1\n",
    "    else:\n",
    "        \n",
    "        if count >= 2:\n",
    "            #import pdb; pdb.set_trace()\n",
    "            MinClusTime = sampled_df['Timestamp'][row-count]\n",
    "            MaxClusTime = sampled_df['Timestamp'][row-1]\n",
    "            k = MaxClusTime - MinClusTime\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            \n",
    "            if (l >= 10):\n",
    "                sampled_df.loc[ (sampled_df['ClusterId']==currcluster), 'StayPoint'] = 1\n",
    "        count = 1\n",
    "        sampled_df['ClusterMeanLat'][row] = sampled_df['Latitude'][row]\n",
    "        sampled_df['ClusterMeanLon'][row] = sampled_df['Longitude'][row]\n",
    "        sampled_df['ClusterId'][row] = currcluster + 1\n",
    "    return count\n",
    "\n",
    "#-----------------------------------FIND STAYPOINTS USING CLUSTERS---------------------------------------------------\n",
    "\n",
    "def find_stay_points():\n",
    "    global sampled_df \n",
    "    global staypts_df\n",
    "    \n",
    "    #Read the file in an online manner as the points come and assign the points to clusters\n",
    "    row =1\n",
    "    count = 1\n",
    "    sampled_df['ClusterId'][row-1] = 0\n",
    "    sampled_df['ClusterMeanLat'][row-1] = sampled_df['Latitude'][0]\n",
    "    sampled_df['ClusterMeanLon'][row-1] = sampled_df['Longitude'][0]\n",
    "    sampled_df['StayPoint'][row-1] = -1\n",
    "    sampled_df['SignificantPlace'][row-1] = -1\n",
    "    while row < len(sampled_df):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        count = cluster(sampled_df['Latitude'][row], sampled_df['Longitude'][row], row, count)\n",
    "        row= row + 1\n",
    "\n",
    "    #copy the stay points into another dataframe\n",
    "    staypts_df = sampled_df.loc[sampled_df['StayPoint'] == 1]\n",
    "    \n",
    "#-----------------------------------GROUP CLUSTERS ON DIFFERENT DAYS-----------------------------------------------\n",
    "\n",
    "def group_clusters():\n",
    "    global staypts_df\n",
    "    \n",
    "    #this fucntion groups the clusters together from different days \n",
    "    #Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    staypts_df1 = staypts_df[['ClusterId', 'ClusterMeanLat', 'ClusterMeanLon']].copy()\n",
    "    staypts_df1 = staypts_df1.drop_duplicates(subset=['ClusterId', 'ClusterMeanLat', 'ClusterMeanLon'])\n",
    "\n",
    "    staypts_df1 = staypts_df1.sort_values(['ClusterMeanLat', 'ClusterMeanLon'])\n",
    "\n",
    "    row = 1\n",
    "    for i in range(0, len(staypts_df1)):\n",
    "        for j in range(i+1, len(staypts_df1)):\n",
    "            #import pdb; pdb.set_trace()\n",
    "        \n",
    "            chk_cluster = staypts_df1['ClusterId'][i]\n",
    "            chk_clulat = staypts_df1['ClusterMeanLat'][i]\n",
    "            chk_clulon = staypts_df1['ClusterMeanLon'][i]\n",
    "            curr_cluster = staypts_df1['ClusterId'][j]\n",
    "            curr_clulat = staypts_df1['ClusterMeanLat'][j]\n",
    "            curr_clulon = staypts_df1['ClusterMeanLon'][j]\n",
    "        \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= 50:\n",
    "                staypts_df.loc[ (staypts_df['ClusterId']==curr_cluster), 'ClusterId'] = chk_cluster\n",
    "                staypts_df['ClusterMeanLat'] = staypts_df.groupby('ClusterId')['ClusterMeanLat'].transform(np.mean)\n",
    "                staypts_df['ClusterMeanLon'] = staypts_df.groupby('ClusterId')['ClusterMeanLon'].transform(np.mean)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "#-----------------------------------SAVE STAYPOINTS AS FILE-----------------------------------------------------\n",
    "\n",
    "def save_file_stay_points():\n",
    "    #Save the file to a location for further analysis\n",
    "    staypts_df.to_csv(dest_file_staypoints, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#------------------------------------VISUALIZE STAYPOINTS-------------------------------------------------------\n",
    "\n",
    "def visualize_stay_points():\n",
    "    global staypts_df\n",
    "    #Plot the significant places\n",
    "\n",
    "#     #load the staypoints file\n",
    "#     staypts_df = pd.read_csv(dest_file_staypoints, sep='\\t')\n",
    "\n",
    "    #make index as datetime\n",
    "    staypts_df.index = pd.to_datetime(staypts_df.Timestamp)\n",
    "\n",
    "    #assign the first lat and log as the base for the plot i.e. origin\n",
    "    origin_lat = math.radians(combined_df[\"Latitude\"][0])\n",
    "    origin_lon = math.radians(combined_df[\"Longitude\"][0])\n",
    "    staypts_df['Timestamp'] = staypts_df.index\n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "\n",
    "    #convert each lat and lon into x and y for the plot w.r.t origin\n",
    "    EARTH_RAD = 6378100\n",
    "    staypts_df['X'] = 0.0\n",
    "    staypts_df['Y'] = 0.0\n",
    "    for i in range(0, len(staypts_df)):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        current_lat = math.radians(staypts_df[\"Latitude\"][i])\n",
    "        current_lon = math.radians(staypts_df[\"Longitude\"][i])\n",
    "        x = ((math.cos(current_lat) + math.cos(origin_lat))/2) * EARTH_RAD * (current_lon - origin_lon) * math.pi / 180\n",
    "        y = (current_lat - origin_lat)* math.pi/180 * EARTH_RAD\n",
    "        staypts_df.at[i, 'X'] = x\n",
    "        staypts_df.at[i, 'Y'] = y\n",
    "\n",
    "    #plot the x and y's\n",
    "    staypts_copy_df = staypts_df[['X', 'Y', 'Timestamp']].copy()\n",
    "    staypts_copy_df['Hour'] = staypts_copy_df.Timestamp.dt.hour\n",
    "    del staypts_copy_df['Timestamp']\n",
    "    #xy_test_df.head()\n",
    "    fg = sns.FacetGrid(data=staypts_copy_df, hue='Hour',   size = 10, aspect = 2)\n",
    "    fg.map(plt.scatter, 'X', 'Y', edgecolor=\"w\", s = 400).add_legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title(\"User Staypoints Trend Plot\")\n",
    "    xticks = np.arange(-1000,1000,50)\n",
    "    yticks = np.arange(-1000,1000,50)\n",
    "    fg.set(xticks=xticks, yticks=yticks)\n",
    "    plt.show()\n",
    "    \n",
    "    staypts_df.index = pd.to_datetime(staypts_df.Timestamp)\n",
    "#-----------------------------------CALCULATE HOURLY WEIGHTS FOR STAYPOINTS--------------------------------------\n",
    "def cal_hourly_cluster_weight():\n",
    "    global staypts_df\n",
    "    global cluster_hourly_df    \n",
    "\n",
    "    for i in range(0, 24):\n",
    "        cluster_hourly_df['Date'] = 0\n",
    "        cluster_hourly_df['ClusterId'] = 0\n",
    "        cluster_hourly_df['AvgLat'] = 0\n",
    "        cluster_hourly_df['AvgLon'] = 0\n",
    "        cluster_hourly_df[i] = 0\n",
    "\n",
    "    last_hour = staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = staypts_df['ClusterId'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(0, len(staypts_df)):\n",
    "\n",
    "        if (i == len(staypts_df)-1):\n",
    "            \n",
    "            k = staypts_df['Timestamp'][i] - staypts_df['Timestamp'][i-curr_count+1]\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            \n",
    "            date_read = staypts_df.index[i].date()\n",
    "            cluster_id = staypts_df['ClusterId'][i]\n",
    "            ClusterMeanLat = staypts_df['ClusterMeanLat'][i]\n",
    "            ClusterMeanLon = staypts_df['ClusterMeanLon'][i]\n",
    "            col_name = staypts_df.index[i].hour\n",
    "\n",
    "            cluster_hourly_df.loc[j,'AvgLat'] = ClusterMeanLat\n",
    "            cluster_hourly_df.loc[j,'AvgLon'] = ClusterMeanLon\n",
    "            cluster_hourly_df.loc[j,'Date'] = date_read\n",
    "            cluster_hourly_df.loc[j,'ClusterId'] = cluster_id\n",
    "            cluster_hourly_df.loc[j, col_name] = round((l)/60,4)\n",
    "            \n",
    "        if (staypts_df['Timestamp'][i].hour != last_hour) | (staypts_df['ClusterId'][i] != last_clusid):\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "            if (curr_count == 1) & (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                k = ((staypts_df['Timestamp'][i-1] + pd.Timedelta(hours=1) - \n",
    "                      pd.Timedelta(minutes=staypts_df['Timestamp'][i-1].minute)) - \n",
    "                     staypts_df['Timestamp'][i-1])\n",
    "            else:\n",
    "                k = staypts_df['Timestamp'][i-1] - staypts_df['Timestamp'][i-curr_count]\n",
    "\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            date_read = staypts_df.index[i-1].date()\n",
    "            cluster_id = staypts_df['ClusterId'][i-1]\n",
    "            ClusterMeanLat = staypts_df['ClusterMeanLat'][i-1]\n",
    "            ClusterMeanLon = staypts_df['ClusterMeanLon'][i-1]\n",
    "            col_name = staypts_df.index[i-1].hour\n",
    "\n",
    "            cluster_hourly_df.loc[j, 'AvgLat'] = ClusterMeanLat\n",
    "            cluster_hourly_df.loc[j, 'AvgLon'] = ClusterMeanLon\n",
    "            cluster_hourly_df.loc[j, 'Date'] = date_read\n",
    "            cluster_hourly_df.loc[j, 'ClusterId'] = cluster_id\n",
    "            cluster_hourly_df.loc[j, col_name] = round((l)/60,4)\n",
    "            j = j + 1\n",
    "            curr_count = 1\n",
    "\n",
    "            if (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                last_hour = staypts_df['Timestamp'][i].hour\n",
    "            if (staypts_df['ClusterId'][i] != last_clusid):\n",
    "                last_clusid = staypts_df['ClusterId'][i]\n",
    "        else:\n",
    "            curr_count = curr_count + 1\n",
    "\n",
    "    cluster_hourly_df = cluster_hourly_df.fillna(0)\n",
    "    cluster_hourly_df = cluster_hourly_df.groupby(['Date', 'ClusterId', 'AvgLat', 'AvgLon']).sum()\n",
    "    cluster_hourly_df = cluster_hourly_df.reset_index(level=[0,1,2,3])\n",
    "\n",
    "#--------------------------------VISUALIZE HOURLY CLUSTER WEIGHT-------------------------------------------------\n",
    "def visualize_hourly_cluster_weight():\n",
    "    global staypts_df\n",
    "    \n",
    "    #create a color dictionary for each cluster for the plot\n",
    "    dicts = {}\n",
    "    clu_list = []\n",
    "    clu_list = staypts_df['ClusterId'].unique()\n",
    "    #r = lambda: random.randint(0,255)\n",
    "    colors = sns.color_palette(\"Paired\", len(clu_list))\n",
    "    \n",
    "    for i in range(0, len(clu_list)):\n",
    "        dicts[clu_list[i]] = (colors[i])\n",
    "        #dicts[clu_list[i]] = ('#%02X%02X%02X' % (r(),r(),r()))\n",
    "        \n",
    "    #create a new graph where we will later add rectangles for each hour:cluster\n",
    "    fig2 = plt.figure(figsize=(15,15))\n",
    "    ax1 = fig2.add_subplot(111, aspect='equal')\n",
    "\n",
    "    #get all the dates for y axis\n",
    "    date_list = staypts_df['Timestamp'].dt.date.unique()\n",
    "    y = range(0, len(date_list))\n",
    "    def_yticks = date_list\n",
    "    plt.yticks(y, def_yticks)\n",
    "    \n",
    "    #set the x axis limit from 0-24 hours of a day, y axis with dates\n",
    "    limsx = (0, 24)\n",
    "    limsy = (0, len(date_list))\n",
    "\n",
    "    date_counter = 0\n",
    "    last_date = staypts_df['Timestamp'][0].date()\n",
    "    last_hour = staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = staypts_df['ClusterId'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        ax1.axvline(x= i, linewidth=1, color='r')\n",
    "\n",
    "    for i in range(0, len(staypts_df)):\n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        #plot a rectangle if the hour or clusterid or date has changed\n",
    "        if ((staypts_df['Timestamp'][i].hour != last_hour) | (staypts_df['ClusterId'][i] != last_clusid)\n",
    "           | (last_date != staypts_df['Timestamp'][i].date())):\n",
    "\n",
    "            if (curr_count == 1) & (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                a = staypts_df['Timestamp'][i-curr_count].hour + 1\n",
    "            else:\n",
    "                a = staypts_df['Timestamp'][i-curr_count].hour + staypts_df['Timestamp'][i-curr_count].minute/60\n",
    "\n",
    "            b = staypts_df['Timestamp'][i-1].hour + staypts_df['Timestamp'][i-1].minute/60\n",
    "\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            col_id = dicts.get(staypts_df['ClusterId'][i-1])\n",
    "            ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=staypts_df['ClusterId'][i-1]))\n",
    "\n",
    "            curr_count = 1\n",
    "\n",
    "            if (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                last_hour = staypts_df['Timestamp'][i].hour\n",
    "            if (staypts_df['ClusterId'][i] != last_clusid):\n",
    "                last_clusid = staypts_df['ClusterId'][i]\n",
    "            if (last_date != staypts_df['Timestamp'][i].date()):\n",
    "                date_counter = date_counter + 1\n",
    "                last_date = staypts_df['Timestamp'][i].date()\n",
    "                ax1.axhline(y= date_counter, linewidth=1, color='r')\n",
    "\n",
    "        else:\n",
    "            curr_count = curr_count + 1\n",
    "            \n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handle_list, label_list = [], []\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if label not in label_list:\n",
    "            handle_list.append(handle)\n",
    "            label_list.append(label)\n",
    "    plt.legend(handle_list, label_list)\n",
    "\n",
    "    plt.xlim(limsx)\n",
    "    plt.ylim(limsy)\n",
    "#-----------------------------------SAVE HOURLY CLUSTER FILE-----------------------------------------------------\n",
    "\n",
    "def save_file_hourly_weights():\n",
    "    global cluster_hourly_df\n",
    "    \n",
    "    #Save the file to a location for further analysis\n",
    "    cluster_hourly_df.to_csv(dest_file_hourly_weights, sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "    \n",
    "#-----------------------------------CREATE TRANSITION MATRICES-----------------------------------------------------\n",
    "\n",
    "def create_save_seperate_trasition_matrices():\n",
    "    #extract unique date list\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    \n",
    "    #create a temp dataframe for each data, and calculate trasition matrices from hour t to t+1\n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "        temp_df = pd.DataFrame()\n",
    "        matrices_df = pd.DataFrame()\n",
    "        temp_df = cluster_hourly_df.loc[cluster_hourly_df['Date'] == date_list[p]]\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "\n",
    "        for i in range(0, 24):\n",
    "            matrices_df['Date'] = 0\n",
    "            matrices_df['ClusterId'] = 0\n",
    "            for j in range(0, len(temp_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['ClusterId'][j])\n",
    "                matrices_df[colname] = 0\n",
    "\n",
    "        matrices_df['Date'] = temp_df['Date']\n",
    "        matrices_df['ClusterId'] = temp_df['ClusterId']\n",
    "        #import pdb; pdb.set_trace()\n",
    "        for i in range (0, 23):\n",
    "            for j in range (0, len(temp_df)):\n",
    "                for k in range (0, len(temp_df)):\n",
    "                    prob = temp_df[i][j] * temp_df[i+1][k]\n",
    "                    colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['ClusterId'][k])\n",
    "                    matrices_df[colname][j] = prob\n",
    "        file_name = dest_path_each_day_trsn_mat + str(date_list[p]) + \".csv\"\n",
    "        matrices_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#---------------------------------------------MARKOV CHAINS-----------------------------------------------------\n",
    "def create_save_markov_chains():\n",
    "    \n",
    "    #create an empty markov chain frame for each clusterid as state, and transition for each hour of the day\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    cluster_list = cluster_hourly_df['ClusterId'].unique()\n",
    "    final_transition_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        final_transition_df['ClusterId'] = 0\n",
    "        for j in range(0, cluster_hourly_df['ClusterId'].nunique()):\n",
    "            colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(cluster_list[j])\n",
    "            final_transition_df[colname] = 0\n",
    "    final_transition_df['ClusterId'] = cluster_list\n",
    "    final_transition_df = final_transition_df.fillna(0)\n",
    "    final_transition_df.index = final_transition_df.ClusterId\n",
    "\n",
    "    #read each day file and sum the matching rows:cols combinations\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    path_dir = dest_path_each_day_trsn_mat\n",
    "\n",
    "    \n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "        temp_df = pd.DataFrame()\n",
    "        filename = path_dir + str(date_list[p]) + '.csv'\n",
    "        temp_df =  pd.read_csv(filename, header = 0, sep='\\t')\n",
    "\n",
    "        for i in range(0, len(temp_df)):\n",
    "            rowname = temp_df['ClusterId'][i]\n",
    "            for src_column in temp_df:\n",
    "                for dest_column in final_transition_df:\n",
    "                    if src_column == dest_column and src_column != 'ClusterId' :\n",
    "                        #import pdb; pdb.set_trace()\n",
    "                        final_transition_df[dest_column][rowname] = (final_transition_df[dest_column][rowname] +\n",
    "                                                                    temp_df[src_column][i])\n",
    "\n",
    "    #calculate probability from cluster x to cluster y from time t to t+1\n",
    "    final_transition_df = final_transition_df.reset_index(drop=True)\n",
    "    for clus in range(0, len(final_transition_df)):\n",
    "        for i in range(0, 24):\n",
    "            temp_sum = 0\n",
    "            for j in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['ClusterId'][j])\n",
    "                temp_sum += (final_transition_df[colname][clus])\n",
    "            for k in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['ClusterId'][k])\n",
    "                if temp_sum != 0:\n",
    "                    final_transition_df[colname][clus] = final_transition_df[colname][clus]/temp_sum\n",
    "\n",
    "    \n",
    "    #replace zero probabilities to a small value and save the file\n",
    "    final_transition_df = final_transition_df.fillna(0)                    \n",
    "    final_transition_df = final_transition_df.replace(0, 0.00001)\n",
    "    final_transition_df.to_csv(dest_file_final_markov_chain, sep='\\t')\n",
    "\n",
    "#---------------------------------------------- S  T  A  R  T  -------------------------------------------------\n",
    "#Start of the program\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "#global dataframes used\n",
    "combined_df = pd.DataFrame()\n",
    "sampled_df = pd.DataFrame()\n",
    "staypts_df = pd.DataFrame()\n",
    "cluster_hourly_df = pd.DataFrame()\n",
    "\n",
    "#source, destination path and filter till date(till when the data needs to be considered). Change according to your needs\n",
    "from_date = \"2009-04-01 00:00:00\"\n",
    "to_date = \"2009-04-30 00:00:00\"\n",
    "fltr_data_date_rng = \"YES\"\n",
    "user = \"001\"\n",
    "\n",
    "\n",
    "usr_directory = \"/home/shashank/Documents/location/Data/User \" + user\n",
    "usr_hrly_wght_directory = \"/home/shashank/Documents/location/Data/User \" + user + \"/hourlyweights\"\n",
    "usr_sty_pts_directory = \"/home/shashank/Documents/location/Data/User \" + user + \"/staypoints\"\n",
    "\n",
    "if not os.path.exists(usr_directory):\n",
    "    os.makedirs(usr_directory)\n",
    "if not os.path.exists(usr_hrly_wght_directory):\n",
    "    os.makedirs(usr_hrly_wght_directory)\n",
    "if not os.path.exists(usr_sty_pts_directory):\n",
    "    os.makedirs(usr_sty_pts_directory)    \n",
    "\n",
    "    \n",
    "file_source_raw = \"/home/shashank/Documents/location/Geolife Trajectories 1.3/Data/\" + user + \"/Trajectory/200*.plt\" \n",
    "dest_file_staypoints = usr_sty_pts_directory + \"/\" + from_date + \"-\" + to_date + \".csv\"\n",
    "dest_file_hourly_weights = usr_hrly_wght_directory + \"/\" + from_date + \"-\" + to_date + \".csv\"\n",
    "dest_path_each_day_trsn_mat = usr_hrly_wght_directory + \"/\"\n",
    "dest_file_final_markov_chain = usr_hrly_wght_directory + \"/final.csv\"\n",
    "\n",
    "load_user_file()\n",
    "\n",
    "resample_select_data()\n",
    "\n",
    "if sampled_df.empty:\n",
    "    print('No data to be processed. Change user or date range and try again.')\n",
    "else:\n",
    "    visualize_raw_data()\n",
    "\n",
    "    find_stay_points()\n",
    "\n",
    "    group_clusters()\n",
    "\n",
    "    visualize_stay_points()\n",
    "\n",
    "    cal_hourly_cluster_weight()\n",
    "\n",
    "    visualize_hourly_cluster_weight()\n",
    "\n",
    "    save_file_stay_points()\n",
    "\n",
    "    save_file_hourly_weights()\n",
    "\n",
    "    create_save_seperate_trasition_matrices()\n",
    "\n",
    "    create_save_markov_chains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
