{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabfab50f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabfaa4710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabfc60860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabf41ff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabfac8ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabfb19da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabf7b6198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabf7bbd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabfc070f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbab9635ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabf83ecf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbab96356a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabfcabef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabf142780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabf142f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabf156898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabfc6b518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabfd18390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbabf906ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import math \n",
    "import os\n",
    "import errno\n",
    "import matplotlib.patches as patches\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "import operator\n",
    "import pdb\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "#-------------------------------\n",
    "def prepare_dfs():\n",
    "    global cluster_hourly_df    \n",
    "    \n",
    "    #create cluster_hourly_df columns\n",
    "    for i in range(0, 24):\n",
    "        cluster_hourly_df['Date'] = 0\n",
    "        cluster_hourly_df['ClusterId'] = 0\n",
    "        cluster_hourly_df['AvgLat'] = 0\n",
    "        cluster_hourly_df['AvgLon'] = 0\n",
    "        cluster_hourly_df[i] = 0\n",
    "        \n",
    "#------------------------------------------------------------------------------------\n",
    "#Find distance between two lan:lon points in meters\n",
    "def meters(lat1, lon1, lat2, lon2):  \n",
    "    R = 6378.137 # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat/2) * math.sin(dLat/2) + math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * math.sin(dLon/2) * math.sin(dLon/2);\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "    d = R * c\n",
    "    return d * 1000 # meters\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "def find_best_k(X):\n",
    "    MAX_CLUSTERS = len(X)\n",
    "\n",
    "    # Start clustering number hashes using temporal features\n",
    "    k_fitted_models = []\n",
    "    for k in range(1, MAX_CLUSTERS+1):\n",
    "        if(k > len(X)):\n",
    "            break\n",
    "        algo = KMeans(n_clusters = k, init='k-means++', verbose = 0)\n",
    "        fitted = algo.fit(X)\n",
    "        k_fitted_models.append(fitted)\n",
    "\n",
    "    # Compute best k value based the elbow method\n",
    "    inertias = np.asarray([fitted.inertia_ for fitted in k_fitted_models])\n",
    "    gradients = -np.diff(np.asarray([fitted.inertia_ for fitted in k_fitted_models]))\n",
    "\n",
    "    delta_x = inertias[0]/MAX_CLUSTERS\n",
    "\n",
    "    sum_couples = [(sum(gradients[0:k]), inertias[k]) for k in range(1,len(gradients))]\n",
    "    atan_couples = [(sum_couples[k][0]/((k+1)*delta_x), sum_couples[k][1]/((len(gradients)-(k+1))*delta_x)) for k in range(len(sum_couples))]\n",
    "    angle_couples = [(math.atan(atan_couples[k][0])*180/math.pi, math.atan(atan_couples[k][1])*180/math.pi) for k in range(0,len(atan_couples))]\n",
    "    elbow_angles= [180-(angle_couples[k][0]-angle_couples[k][1]) for k in range(0,len(angle_couples))]\n",
    "\n",
    "    best_k = min(enumerate(elbow_angles), key=operator.itemgetter(1))[0]+1\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    return best_k\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "def k_mean_on_stay_points():\n",
    "    global staypts_df\n",
    "    global cluster_hourly_df\n",
    "    global clus_dict\n",
    "    \n",
    "    data = cluster_hourly_df[[\"ClusterId\", \"AvgLat\", \"AvgLon\"]]\n",
    "    #data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    #calculate x and y from Avg Lat and Avg Lon\n",
    "    #assign the first lat and log as the base for the plot i.e. origin\n",
    "    origin_lat = math.radians(staypts_df[\"Latitude\"][0])\n",
    "    origin_lon = math.radians(staypts_df[\"Longitude\"][0])\n",
    "    \n",
    "    #convert each lat and lon into x and y \n",
    "    EARTH_RAD = 6378100\n",
    "    data['X'] = 0.0\n",
    "    data['Y'] = 0.0\n",
    "    for i in range(0, len(data)):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        current_lat = math.radians(data[\"AvgLat\"][i])\n",
    "        current_lon = math.radians(data[\"AvgLon\"][i])\n",
    "        x = ((math.cos(current_lat) + math.cos(origin_lat))/2) * EARTH_RAD * (current_lon - origin_lon) * math.pi / 180\n",
    "        y = (current_lat - origin_lat)* math.pi/180 * EARTH_RAD\n",
    "        data.at[i, 'X'] = x\n",
    "        data.at[i, 'Y'] = y\n",
    "        \n",
    "    #import pdb; pdb.set_trace()\n",
    "    f1 = data['X'].values\n",
    "    f2 = data['Y'].values\n",
    "    X = np.array(list(zip(f1, f2)))\n",
    "    \n",
    "    if len(X) <= 1:\n",
    "        return\n",
    "    \n",
    "    # Best number of clusters\n",
    "    for k in range(1, len(X)+1):\n",
    "        recluster = 0\n",
    "        # Number of clusters\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        # Fitting the input data\n",
    "        kmeans = kmeans.fit(X)\n",
    "        # Getting the cluster labels\n",
    "        labels = kmeans.predict(X)\n",
    "\n",
    "        for i in range(k):\n",
    "            points = np.array([X[j] for j in range(len(X)) if labels[j] == i])\n",
    "            x2 = max(points[:, 0])\n",
    "            x1 = min(points[:, 0])\n",
    "            y2 = max(points[:, 1])\n",
    "            y1 = min(points[:, 1])\n",
    "            \n",
    "            #if any cluster has diamiter greater than 100, then increase k and recluster\n",
    "            if ((x2 - x1) >= 100) or ((y2 - y1) >= 100):    \n",
    "                recluster = 1\n",
    "        \n",
    "        if recluster == 0:\n",
    "            best_k = k\n",
    "            break\n",
    "   \n",
    "    #adding the cluster labels as new cluster ids\n",
    "    data[\"NewClusterId\"] = labels + 1\n",
    "    \n",
    "    colors = sns.color_palette(\"nipy_spectral_r\", k)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), dpi=80)\n",
    "\n",
    "    for i in range(best_k):\n",
    "        points = np.array([X[j] for j in range(len(X)) if labels[j] == i])\n",
    "        x2 = max(points[:, 0])\n",
    "        x1 = min(points[:, 0])\n",
    "        y2 = max(points[:, 1])\n",
    "        y1 = min(points[:, 1])\n",
    "        #import pdb; pdb.set_trace()\n",
    "        centre_x = (x2+x1)/2\n",
    "        centre_y = (y2+y1)/2\n",
    "        \n",
    "        ellipse = Ellipse(xy=(centre_x, centre_y), width=(x2-x1), height=(y2-y1), edgecolor=colors[i], fc='None', lw=2)\n",
    "        ax.add_patch(ellipse)\n",
    "        #circ = Circle((centre_x, centre_y), rad+5, edgecolor=colors[i], fc='None', lw=2)\n",
    "        #ax.add_patch(circ)\n",
    "        ax.scatter(points[:, 0], points[:, 1], s=100, edgecolor=colors[i], alpha=0.5, c = colors[i])\n",
    "    \n",
    "    #plt.xlim(-500, 500)\n",
    "    #plt.ylim(-500, 500)\n",
    "    plt.show()\n",
    "    \n",
    "    #create a dictionary for old-new cluster ids\n",
    "    clus_dict = pd.Series(data.NewClusterId.values,index=data.ClusterId).to_dict()\n",
    "    \n",
    "    #update for new cluster ids in all dfs\n",
    "    for i in range(0, len(cluster_hourly_df)):\n",
    "        #cluster_hourly_df[\"DistanceCluster\"][i] = clus_dict.get(cluster_hourly_df[\"ClusterId\"][i])\n",
    "        cluster_hourly_df[\"ClusterId\"][i] = clus_dict.get(cluster_hourly_df[\"ClusterId\"][i])\n",
    "                                                          \n",
    "    cluster_hourly_df['AvgLat'] = cluster_hourly_df.groupby('ClusterId')['AvgLat'].transform(np.mean)\n",
    "    cluster_hourly_df['AvgLon'] = cluster_hourly_df.groupby('ClusterId')['AvgLon'].transform(np.mean)\n",
    "    cluster_hourly_df = cluster_hourly_df.groupby(['Date', 'ClusterId', 'AvgLat', 'AvgLon']).sum()\n",
    "    cluster_hourly_df = cluster_hourly_df.reset_index(level=[0,1,2,3])\n",
    "    \n",
    "    for i in range(0, len(staypts_df)):\n",
    "        #staypts_df[\"DistanceCluster\"][i] = clus_dict.get(staypts_df[\"ClusterId\"][i])\n",
    "        staypts_df[\"ClusterId\"][i] = clus_dict.get(staypts_df[\"ClusterId\"][i])\n",
    "    \n",
    "    staypts_df['ClusterMeanLat'] = staypts_df.groupby('ClusterId')['Latitude'].transform(np.mean)\n",
    "    staypts_df['ClusterMeanLon'] = staypts_df.groupby('ClusterId')['Longitude'].transform(np.mean)\n",
    "    \n",
    "    \n",
    "#------------------------------------------------------------------------------------\n",
    "def cluster(newlat, newlon, row, count):\n",
    "    global curr_hr_df\n",
    "    \n",
    "    currcluster = curr_hr_df['ClusterId'][row-1]\n",
    "    curr_hr_df['ClusterId'][row] = -1\n",
    "    curr_hr_df['ClusterMeanLat'][row] = -1.0\n",
    "    curr_hr_df['ClusterMeanLon'][row] = -1.0\n",
    "    curr_hr_df['StayPoint'][row] = -1\n",
    "    clulat = curr_hr_df['ClusterMeanLat'][row-1]\n",
    "    clulon = curr_hr_df['ClusterMeanLon'][row-1]\n",
    "    \n",
    "    if meters(clulat, clulon, newlat, newlon)<= 50:\n",
    "        curr_hr_df['ClusterId'][row] = currcluster\n",
    "        curr_hr_df['ClusterMeanLat'] = curr_hr_df.groupby('ClusterId')['Latitude'].transform(np.mean)\n",
    "        curr_hr_df['ClusterMeanLon'] = curr_hr_df.groupby('ClusterId')['Longitude'].transform(np.mean)\n",
    "        count = count + 1\n",
    "    else:\n",
    "        \n",
    "        if count >= 2:\n",
    "            MinClusTime = curr_hr_df['Timestamp'][row-count]\n",
    "            MaxClusTime = curr_hr_df['Timestamp'][row-1]\n",
    "            k = MaxClusTime - MinClusTime\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            \n",
    "            if (l >= 10):\n",
    "                curr_hr_df.loc[ (curr_hr_df['ClusterId']==currcluster), 'StayPoint'] = 1\n",
    "        count = 1\n",
    "        curr_hr_df['ClusterMeanLat'][row] = curr_hr_df['Latitude'][row]\n",
    "        curr_hr_df['ClusterMeanLon'][row] = curr_hr_df['Longitude'][row]\n",
    "        curr_hr_df['ClusterId'][row] = currcluster + 1\n",
    "    return count\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_usr_file():\n",
    "    global usr_trejec_df\n",
    "    \n",
    "    #Load file names for user\n",
    "    filenames = glob.glob(file_source_raw)\n",
    "\n",
    "    #Read the files\n",
    "    list_of_dfs = [pd.read_csv(filename, skiprows=6, header = None) for filename in filenames]\n",
    "\n",
    "    #put the data from list into one dataframe\n",
    "    usr_trejec_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    usr_trejec_df.columns = ['Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "    usr_trejec_df[\"Timestamp\"] = usr_trejec_df[\"Date\"].map(str) + \" \" + usr_trejec_df[\"Time\"]\n",
    "    \n",
    "    usr_trejec_df.index = pd.to_datetime(usr_trejec_df.Timestamp)\n",
    "    usr_trejec_df = usr_trejec_df.resample('1T').mean()\n",
    "    usr_trejec_df = usr_trejec_df.dropna()\n",
    "    \n",
    "     #add columns to user trajectory dataframe\n",
    "    #1. add timestamp as column\n",
    "    usr_trejec_df['Timestamp'] = pd.to_datetime(usr_trejec_df.index)\n",
    "    #restore date and time column\n",
    "    usr_trejec_df['Date'] = usr_trejec_df.Timestamp.dt.date\n",
    "    usr_trejec_df['Time'] = usr_trejec_df.Timestamp.dt.time\n",
    "    #2. add hour as column\n",
    "    usr_trejec_df['Hour'] = usr_trejec_df.Timestamp.dt.hour\n",
    "    #3. add weekday number.name as column\n",
    "    usr_trejec_df['Weekday'] = usr_trejec_df['Timestamp'].dt.weekday.map(str) + usr_trejec_df['Timestamp'].dt.weekday_name\n",
    "    #4. ClusterId, 5. ClusterMeanLat, 6. ClusterMeanLon, 7. StayPoint, 8. DistanceCluster\n",
    "    usr_trejec_df['ClusterId'] = -1\n",
    "    usr_trejec_df['ClusterMeanLat'] = -1.0\n",
    "    usr_trejec_df['ClusterMeanLon'] = -1.0\n",
    "    usr_trejec_df['StayPoint'] = -1\n",
    "    \n",
    "    #remove columns not used\n",
    "    usr_trejec_df = usr_trejec_df.drop(['0', 'Altitude', 'NumDays'], axis = 1)\n",
    "    \n",
    "    #sort the trajectory based on date and time\n",
    "    usr_trejec_df = usr_trejec_df.sort_values(['Date', 'Time'])\n",
    "    \n",
    "    #reset index\n",
    "    usr_trejec_df = usr_trejec_df.reset_index(drop=True)\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_trained_model():\n",
    "    global trained_model_df\n",
    "    \n",
    "    if os.path.isfile(dest_file_final_markov_chain):\n",
    "        trained_model_df = pd.read_csv(dest_file_final_markov_chain, header = 0, sep=\"\\t\")\n",
    "    \n",
    "#------------------------------------------------------------------------------------\n",
    "def create_last_hr_staypts():\n",
    "    global curr_hr_df \n",
    "    global staypts_df\n",
    "    global curr_hr_staypts_df\n",
    "    \n",
    "    #clear current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_staypts_df.iloc[0:0]\n",
    "    \n",
    "    #reset index\n",
    "    curr_hr_df = curr_hr_df.reset_index(drop=True)\n",
    "    \n",
    "    #Read the file in an online manner as the points come and assign the points to clusters\n",
    "    row =1\n",
    "    count = 1\n",
    "    if not staypts_df.empty:\n",
    "        curr_hr_df['ClusterId'][row-1] = staypts_df['ClusterId'].max() + 1\n",
    "    else:\n",
    "        curr_hr_df['ClusterId'][row-1] = 1\n",
    "    \n",
    "    curr_hr_df['ClusterMeanLat'][row-1] = curr_hr_df['Latitude'][0]\n",
    "    curr_hr_df['ClusterMeanLon'][row-1] = curr_hr_df['Longitude'][0]\n",
    "    curr_hr_df['StayPoint'][row-1] = -1\n",
    "    \n",
    "    while row < len(curr_hr_df):\n",
    "        count = cluster(curr_hr_df['Latitude'][row], curr_hr_df['Longitude'][row], row, count)\n",
    "        row= row + 1\n",
    "    \n",
    "    #copy the staypoints to the current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_df.loc[curr_hr_df['StayPoint'] == 1]\n",
    "    #copy the stay points into another dataframe\n",
    "    staypts_df = staypts_df.append(curr_hr_df.loc[curr_hr_df['StayPoint'] == 1])\n",
    "    #reset staypoints index\n",
    "    curr_hr_staypts_df.index = curr_hr_staypts_df['Timestamp']\n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "    #clear current hour dataframe\n",
    "    curr_hr_df = curr_hr_df.iloc[0:0]\n",
    "    \n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "def cal_hourly_cluster_weight():\n",
    "    global curr_hr_staypts_df\n",
    "    global cluster_hourly_df   \n",
    "    \n",
    "    curr_hr_cluster_hourly_df = pd.DataFrame()       \n",
    "    \n",
    "    last_hour = curr_hr_staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = curr_hr_staypts_df['ClusterId'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        curr_hr_cluster_hourly_df['Date'] = 0\n",
    "        curr_hr_cluster_hourly_df['ClusterId'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLat'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLon'] = 0\n",
    "        curr_hr_cluster_hourly_df[i] = 0\n",
    "    \n",
    "    for i in range(0, len(curr_hr_staypts_df)):\n",
    "\n",
    "        if (i == len(curr_hr_staypts_df)-1):\n",
    "            \n",
    "            k = curr_hr_staypts_df['Timestamp'][i] - curr_hr_staypts_df['Timestamp'][i-curr_count]\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            \n",
    "            date_read = curr_hr_staypts_df.index[i].date()\n",
    "            cluster_id = curr_hr_staypts_df['ClusterId'][i]\n",
    "            ClusterMeanLat = curr_hr_staypts_df['ClusterMeanLat'][i]\n",
    "            ClusterMeanLon = curr_hr_staypts_df['ClusterMeanLon'][i]\n",
    "            col_name = curr_hr_staypts_df.index[i].hour\n",
    "\n",
    "            curr_hr_cluster_hourly_df.loc[j,'AvgLat'] = ClusterMeanLat\n",
    "            curr_hr_cluster_hourly_df.loc[j,'AvgLon'] = ClusterMeanLon\n",
    "            curr_hr_cluster_hourly_df.loc[j,'Date'] = date_read\n",
    "            curr_hr_cluster_hourly_df.loc[j,'ClusterId'] = cluster_id\n",
    "            curr_hr_cluster_hourly_df.loc[j, col_name] = round((l)/60,4)\n",
    "            \n",
    "        if (curr_hr_staypts_df['Timestamp'][i].hour != last_hour) | (curr_hr_staypts_df['ClusterId'][i] != last_clusid):\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "            if (curr_count == 1) & (curr_hr_staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                k = ((curr_hr_staypts_df['Timestamp'][i-1] + pd.Timedelta(hours=1) - \n",
    "                      pd.Timedelta(minutes=curr_hr_staypts_df['Timestamp'][i-1].minute)) - \n",
    "                     curr_hr_staypts_df['Timestamp'][i-1])\n",
    "            else:\n",
    "                k = curr_hr_staypts_df['Timestamp'][i-1] - curr_hr_staypts_df['Timestamp'][i-curr_count]\n",
    "\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            date_read = curr_hr_staypts_df.index[i-1].date()\n",
    "            cluster_id = curr_hr_staypts_df['ClusterId'][i-1]\n",
    "            ClusterMeanLat = curr_hr_staypts_df['ClusterMeanLat'][i-1]\n",
    "            ClusterMeanLon = curr_hr_staypts_df['ClusterMeanLon'][i-1]\n",
    "            col_name = curr_hr_staypts_df.index[i-1].hour\n",
    "\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'AvgLat'] = ClusterMeanLat\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'AvgLon'] = ClusterMeanLon\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'Date'] = date_read\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'ClusterId'] = cluster_id\n",
    "            curr_hr_cluster_hourly_df.loc[j, col_name] = round((l)/60,4)\n",
    "            j = j + 1\n",
    "            curr_count = 1\n",
    "\n",
    "            if (curr_hr_staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                last_hour = curr_hr_staypts_df['Timestamp'][i].hour\n",
    "            if (curr_hr_staypts_df['ClusterId'][i] != last_clusid):\n",
    "                last_clusid = curr_hr_staypts_df['ClusterId'][i]\n",
    "        else:\n",
    "            curr_count = curr_count + 1\n",
    "\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.fillna(0)\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.groupby(['Date', 'ClusterId', 'AvgLat', 'AvgLon']).sum()\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.reset_index(level=[0,1,2,3])\n",
    "   \n",
    "    cluster_hourly_df = cluster_hourly_df.append(curr_hr_cluster_hourly_df, ignore_index=True)\n",
    "    cluster_hourly_df = cluster_hourly_df.reset_index(drop=True)\n",
    "    \n",
    "#-------------group_clusters-----------------------------------------------------------------------\n",
    "def group_clusters():\n",
    "    global staypts_df\n",
    "    \n",
    "    #this fucntion groups the clusters together from different days \n",
    "    #Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    staypts_df1 = staypts_df[['ClusterId', 'ClusterMeanLat', 'ClusterMeanLon']].copy()\n",
    "    staypts_df1 = staypts_df1.drop_duplicates(subset=['ClusterId', 'ClusterMeanLat', 'ClusterMeanLon'])\n",
    "\n",
    "    staypts_df1 = staypts_df1.sort_values(['ClusterMeanLat', 'ClusterMeanLon'])\n",
    "\n",
    "    row = 1\n",
    "    for i in range(0, len(staypts_df1)):\n",
    "        for j in range(i+1, len(staypts_df1)):\n",
    "        \n",
    "            chk_cluster = staypts_df1['ClusterId'][i]\n",
    "            chk_clulat = staypts_df1['ClusterMeanLat'][i]\n",
    "            chk_clulon = staypts_df1['ClusterMeanLon'][i]\n",
    "            curr_cluster = staypts_df1['ClusterId'][j]\n",
    "            curr_clulat = staypts_df1['ClusterMeanLat'][j]\n",
    "            curr_clulon = staypts_df1['ClusterMeanLon'][j]\n",
    "        \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= 50:\n",
    "                staypts_df.loc[ (staypts_df['ClusterId']==curr_cluster), 'ClusterId'] = chk_cluster\n",
    "                staypts_df['ClusterMeanLat'] = staypts_df.groupby('ClusterId')['ClusterMeanLat'].transform(np.mean)\n",
    "                staypts_df['ClusterMeanLon'] = staypts_df.groupby('ClusterId')['ClusterMeanLon'].transform(np.mean)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "def visualize_hourly_cluster_weight():\n",
    "    global staypts_df\n",
    "    \n",
    "    #create a color dictionary for each cluster for the plot\n",
    "    dicts = {}\n",
    "    clu_list = []\n",
    "    clu_list = staypts_df['ClusterId'].unique()\n",
    "    #r = lambda: random.randint(0,255)\n",
    "    colors = sns.color_palette(\"Paired\", len(clu_list))\n",
    "    \n",
    "    for i in range(0, len(clu_list)):\n",
    "        dicts[clu_list[i]] = (colors[i])\n",
    "        #dicts[clu_list[i]] = ('#%02X%02X%02X' % (r(),r(),r()))\n",
    "        \n",
    "    #create a new graph where we will later add rectangles for each hour:cluster\n",
    "    fig2 = plt.figure(figsize=(15,15))\n",
    "    ax1 = fig2.add_subplot(111, aspect='equal')\n",
    "\n",
    "    #get all the dates for y axis\n",
    "    date_list = staypts_df['Timestamp'].dt.date.unique()\n",
    "    y = range(0, len(date_list))\n",
    "    def_yticks = date_list\n",
    "    plt.yticks(y, def_yticks)\n",
    "    \n",
    "    #set the x axis limit from 0-24 hours of a day, y axis with dates\n",
    "    limsx = (0, 24)\n",
    "    limsy = (0, len(date_list))\n",
    "\n",
    "    date_counter = 0\n",
    "    last_date = staypts_df['Timestamp'][0].date()\n",
    "    last_hour = staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = staypts_df['ClusterId'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "    \n",
    "    #drawing verical lines for each hour\n",
    "    for i in range(0, 24):\n",
    "        ax1.axvline(x= i, linewidth=1, color='r')\n",
    "\n",
    "    for i in range(0, len(staypts_df)):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        if (i == len(staypts_df)-1):\n",
    "            a = staypts_df['Timestamp'][i-curr_count].hour + staypts_df['Timestamp'][i-curr_count].minute/60\n",
    "            b = staypts_df['Timestamp'][i].hour + staypts_df['Timestamp'][i].minute/60\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            col_id = dicts.get(staypts_df['ClusterId'][i])\n",
    "            ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=staypts_df['ClusterId'][i]))\n",
    "            \n",
    "        #plot a rectangle if the hour or clusterid or date has changed\n",
    "        if ((staypts_df['Timestamp'][i].hour != last_hour) | (staypts_df['ClusterId'][i] != last_clusid)\n",
    "           | (last_date != staypts_df['Timestamp'][i].date())):\n",
    "\n",
    "            if (curr_count == 1) & (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                a = staypts_df['Timestamp'][i-curr_count].hour + 1\n",
    "            else:\n",
    "                a = staypts_df['Timestamp'][i-curr_count].hour + staypts_df['Timestamp'][i-curr_count].minute/60\n",
    "\n",
    "            b = staypts_df['Timestamp'][i-1].hour + staypts_df['Timestamp'][i-1].minute/60\n",
    "\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            col_id = dicts.get(staypts_df['ClusterId'][i-1])\n",
    "            ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=staypts_df['ClusterId'][i-1]))\n",
    "\n",
    "            curr_count = 1\n",
    "\n",
    "            if (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                last_hour = staypts_df['Timestamp'][i].hour\n",
    "            if (staypts_df['ClusterId'][i] != last_clusid):\n",
    "                last_clusid = staypts_df['ClusterId'][i]\n",
    "            if (last_date != staypts_df['Timestamp'][i].date()):\n",
    "                date_counter = date_counter + 1\n",
    "                last_date = staypts_df['Timestamp'][i].date()\n",
    "                ax1.axhline(y= date_counter, linewidth=1, color='r')\n",
    "\n",
    "        else:\n",
    "            curr_count = curr_count + 1\n",
    "            \n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handle_list, label_list = [], []\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if label not in label_list:\n",
    "            handle_list.append(handle)\n",
    "            label_list.append(label)\n",
    "    plt.legend(handle_list, label_list)\n",
    "\n",
    "    plt.xlim(limsx)\n",
    "    plt.ylim(limsy)\n",
    "    plt.show()\n",
    "#-----------------------------------------------------------------------------------\n",
    "def update_staypts_csv():\n",
    "     with open(dest_file_staypoints, 'a') as f:\n",
    "             (staypts_df).to_csv(f,  sep='\\t', encoding='utf-8')\n",
    "#-----------------------------------------------------------------------------------\n",
    "def update_hourly_weights_csv():\n",
    "    with open(dest_file_hourly_weights, 'a') as f:\n",
    "             (cluster_hourly_df).to_csv(f,  sep='\\t', encoding='utf-8')\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_save_seperate_trasition_matrices():\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    \n",
    "    #create a temp dataframe for each data, and calculate trasition matrices from hour t to t+1\n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "\n",
    "        #create a temp dataframe for pervious date\n",
    "        temp_df = pd.DataFrame()\n",
    "        matrices_df = pd.DataFrame()\n",
    "        temp_df = cluster_hourly_df.loc[cluster_hourly_df['Date'] == date_list[p]]\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "\n",
    "        for i in range(0, 24):\n",
    "            matrices_df['Date'] = 0\n",
    "            matrices_df['ClusterId'] = 0\n",
    "            for j in range(0, len(temp_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['ClusterId'][j])\n",
    "                matrices_df[colname] = 0\n",
    "\n",
    "        matrices_df['Date'] = temp_df['Date']\n",
    "        matrices_df['ClusterId'] = temp_df['ClusterId']\n",
    "\n",
    "        for i in range (0, 23):\n",
    "            for j in range (0, len(temp_df)):\n",
    "                for k in range (0, len(temp_df)):\n",
    "                    prob = temp_df[i][j] * temp_df[i+1][k]\n",
    "                    colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['ClusterId'][k])\n",
    "                    matrices_df[colname][j] = prob\n",
    "        file_name = dest_path_each_day_trsn_mat + str(date_list[p]) + \".csv\"\n",
    "        matrices_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_save_markov_chains():\n",
    "    global final_transition_df\n",
    "    global co_loc\n",
    "    \n",
    "    final_transition_df = pd.DataFrame()\n",
    "    \n",
    "    #create an empty markov chain frame for each clusterid as state, and transition for each hour of the day\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    cluster_list = cluster_hourly_df['ClusterId'].unique()\n",
    "    AvgLat_list = cluster_hourly_df['AvgLat'].unique()\n",
    "    AvgLon_list = cluster_hourly_df['AvgLon'].unique()\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        final_transition_df['Address'] = 0\n",
    "        final_transition_df['AvgLat'] = 0\n",
    "        final_transition_df['AvgLon'] = 0\n",
    "        final_transition_df['ClusterId'] = 0\n",
    "        for j in range(0, cluster_hourly_df['ClusterId'].nunique()):\n",
    "            colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(cluster_list[j])\n",
    "            final_transition_df[colname] = 0\n",
    "          \n",
    "    final_transition_df['ClusterId'] = cluster_list\n",
    "    final_transition_df['AvgLat'] = AvgLat_list\n",
    "    final_transition_df['AvgLon'] = AvgLon_list\n",
    "    final_transition_df = final_transition_df.fillna(0)\n",
    "    final_transition_df.index = final_transition_df.ClusterId\n",
    "\n",
    "    #read each day file and sum the matching rows:cols combinations\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    path_dir = dest_path_each_day_trsn_mat\n",
    "\n",
    "    \n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "        temp_df = pd.DataFrame()\n",
    "        filename = path_dir + str(date_list[p]) + '.csv'\n",
    "        temp_df =  pd.read_csv(filename, header = 0, sep='\\t')\n",
    "\n",
    "        for i in range(0, len(temp_df)):\n",
    "            rowname = temp_df['ClusterId'][i]\n",
    "            for src_column in temp_df:\n",
    "                for dest_column in final_transition_df:\n",
    "                    if src_column == dest_column and src_column != 'ClusterId' :\n",
    "                        #import pdb; pdb.set_trace()\n",
    "                        final_transition_df[dest_column][rowname] = (final_transition_df[dest_column][rowname] +\n",
    "                                                                    temp_df[src_column][i])\n",
    "\n",
    "    #calculate probability from cluster x to cluster y from time t to t+1\n",
    "    final_transition_df = final_transition_df.reset_index(drop=True)\n",
    "    for clus in range(0, len(final_transition_df)):\n",
    "        for i in range(0, 24):\n",
    "            temp_sum = 0\n",
    "            for j in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['ClusterId'][j])\n",
    "                temp_sum += (final_transition_df[colname][clus])\n",
    "            for k in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['ClusterId'][k])\n",
    "                if temp_sum != 0:\n",
    "                    final_transition_df[colname][clus] = final_transition_df[colname][clus]/temp_sum\n",
    "    \n",
    "    #create dictionary for coordinate : address\n",
    "    points = tuple(zip(final_transition_df.AvgLat, final_transition_df.AvgLon))\n",
    "    geocoder = Nominatim(timeout=10)\n",
    "    coordinate_location = {}\n",
    "    \n",
    "    for coordinate in points:\n",
    "        try:\n",
    "            location = geocoder.reverse(coordinate)\n",
    "        except:\n",
    "            location = 'unknown'\n",
    "        coordinate_location[coordinate] = location\n",
    "    \n",
    "    co_loc = {k:v for k,v in coordinate_location.items()}\n",
    "\n",
    "    for i in range(0, len(final_transition_df)):\n",
    "        address = co_loc.get((final_transition_df['AvgLat'][i], final_transition_df['AvgLon'][i]))\n",
    "        if address == 'unknown':\n",
    "            final_transition_df['Address'][i] = 'unknown'\n",
    "        else:\n",
    "            final_transition_df['Address'][i] = address[0]\n",
    "    \n",
    "    #replace zero probabilities to a small value and save the file\n",
    "    final_transition_df = final_transition_df.fillna(0)                    \n",
    "    final_transition_df = final_transition_df.replace(0, 0.00001)\n",
    "    final_transition_df.to_csv(dest_file_final_markov_chain, sep='\\t')\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        final_transition_temp_df = pd.DataFrame()\n",
    "        k = cluster_hourly_df['ClusterId'].nunique()*i + 4\n",
    "        final_transition_temp_df = final_transition_df.iloc[:,k:k + cluster_hourly_df['ClusterId'].nunique()]\n",
    "        final_transition_temp_df.index = cluster_list\n",
    "        file_name = usr_markov_chains_directory + \"/\" + str(i) + \" hour.csv\"\n",
    "        final_transition_temp_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "        \n",
    "        \n",
    "#------------------------------------------------------------------------------------\n",
    "def predict(new_hour):\n",
    "    global trained_model_df\n",
    "    global curr_hr_staypts_df\n",
    "    \n",
    "    tobepredicted_df = curr_hr_staypts_df[['ClusterId', 'ClusterMeanLat', 'ClusterMeanLon']]\n",
    "    tobepredicted_df = tobepredicted_df.drop_duplicates()\n",
    "    tobepredicted_df = tobepredicted_df.reset_index(drop=True)\n",
    "\n",
    "    for j in range(0, len(tobepredicted_df)):\n",
    "        new_lat = tobepredicted_df['ClusterMeanLat'][j]\n",
    "        new_lon = tobepredicted_df['ClusterMeanLon'][j]\n",
    "        file_name = \"staypoint - \" +  str(tobepredicted_df['ClusterId'][j]) + \".csv\"\n",
    "        for i in range(0, len(trained_model_df)):\n",
    "\n",
    "            trn_lat = trained_model_df['AvgLat'][i]\n",
    "            trn_lon = trained_model_df['AvgLon'][i]\n",
    "            if meters(trn_lat, trn_lon, new_lat, new_lon) <= 10000:\n",
    "\n",
    "                predic_df = pd.DataFrame()\n",
    "\n",
    "                cluster_id = trained_model_df['ClusterId'][i]\n",
    "                curr_lat = trained_model_df['AvgLat'][i]\n",
    "                curr_lon = trained_model_df['AvgLon'][i]\n",
    "                curr_add = trained_model_df['Address'][i]\n",
    "                pred_loc = {\"current\":(cluster_id, curr_lat, curr_lon, curr_add)}\n",
    "\n",
    "                from_col_no = trained_model_df['ClusterId'].nunique() * new_hour + 5\n",
    "                to_col_no = from_col_no + trained_model_df['ClusterId'].nunique()\n",
    "                predic_df = trained_model_df.iloc[i:i+1,from_col_no:to_col_no]\n",
    "                predic_df = predic_df.T\n",
    "                predic_df['ClusterId'] = cluster_id\n",
    "                predic_df['SelectedCluster'] = predic_df.index\n",
    "                predic_df['SelectedCluster'] = predic_df['SelectedCluster'].map(lambda x: x.split('-', 2)[-1])\n",
    "                predic_df.columns = ['Probability', 'ClusterId', 'SelectedCluster']\n",
    "                predic_df = predic_df.sort_values('Probability', ascending=False).head(10)\n",
    "                predic_df['Address'] = 0\n",
    "                predic_df['Latitude'] = 0.0\n",
    "                predic_df['Longitude'] = 0.0\n",
    "                predic_df = predic_df.reset_index(drop=True)\n",
    "\n",
    "                for j in range (0, len(predic_df)):\n",
    "                    #import pdb; pdb.set_trace()\n",
    "                    clus_to_find = int(float(predic_df['SelectedCluster'][j]))\n",
    "                    add = trained_model_df.loc[ (trained_model_df['ClusterId'] == clus_to_find), 'Address'].values[0]\n",
    "                    lat = trained_model_df.loc[ (trained_model_df['ClusterId'] == clus_to_find), 'AvgLat'].values[0]\n",
    "                    lon = trained_model_df.loc[ (trained_model_df['ClusterId'] == clus_to_find), 'AvgLon'].values[0]\n",
    "\n",
    "                    predic_df.loc[j, 'Address'] = add\n",
    "                    predic_df.loc[j, 'Latitude'] = lat\n",
    "                    predic_df.loc[j, 'Longitude'] = lon\n",
    "                file = dest_predicted_dir + file_name\n",
    "                predic_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "                break\n",
    "            \n",
    "#------------------------------------------ S T A R T -----------------------------------------------\n",
    "#global dataframes used\n",
    "#user raw trajectory dataframe\n",
    "usr_trejec_df = pd.DataFrame()\n",
    "#user trained model\n",
    "trained_model_df = pd.DataFrame()\n",
    "#current hour points\n",
    "curr_hr_df = pd.DataFrame()\n",
    "#current hour staypoints\n",
    "curr_hr_staypts_df = pd.DataFrame()\n",
    "#all staypoints\n",
    "staypts_df = pd.DataFrame()\n",
    "#hourly cluster\n",
    "cluster_hourly_df = pd.DataFrame()\n",
    "#final markov chains\n",
    "final_transition_df = pd.DataFrame()\n",
    "\n",
    "clus_dict = {}\n",
    "co_loc = {}\n",
    "global_clusterid = 0\n",
    "pred_loc = {}\n",
    "lat_array = []\n",
    "lon_array = []\n",
    "global_count = 0\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------CHANGE HERE FOR USER AND DATE RANGE--------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Edit user name, and path locations for source and destination files\n",
    "user = \"041\"\n",
    "\n",
    "\n",
    "#destination paths\n",
    "usr_directory = \"/home/shashank/Documents/location/code/test prediction quality/test 1/results/User \" + user\n",
    "usr_hrly_wght_directory = \"/home/shashank/Documents/location/code/test prediction quality/test 1/results/User \" + user + \"/hourlyweights\"\n",
    "usr_sty_pts_directory = \"/home/shashank/Documents/location/code/test prediction quality/test 1/results/User \" + user + \"/staypoints\"\n",
    "usr_markov_chains_directory = \"/home/shashank/Documents/location/code/test prediction quality/test 1/results/User \" + user + \"/markovchains\"\n",
    "dest_predicted_dir = \"/home/shashank/Documents/location/code/test prediction quality/test 1/results/User \" + user + \"/predict/\"\n",
    "\n",
    "if not os.path.exists(usr_directory):\n",
    "    os.makedirs(usr_directory)\n",
    "if not os.path.exists(usr_hrly_wght_directory):\n",
    "    os.makedirs(usr_hrly_wght_directory)\n",
    "if not os.path.exists(usr_sty_pts_directory):\n",
    "    os.makedirs(usr_sty_pts_directory)  \n",
    "if not os.path.exists(usr_markov_chains_directory):\n",
    "    os.makedirs(usr_markov_chains_directory)  \n",
    "if not os.path.exists(dest_predicted_dir):\n",
    "    os.makedirs(dest_predicted_dir)  \n",
    "\n",
    "#destination file names\n",
    "dest_file_staypoints = usr_sty_pts_directory + \"/staypoints.csv\"\n",
    "dest_file_hourly_weights = usr_hrly_wght_directory + \"/hourlyweights.csv\"\n",
    "dest_path_each_day_trsn_mat = usr_hrly_wght_directory + \"/\"\n",
    "dest_file_final_markov_chain = usr_markov_chains_directory + \"/final.csv\"\n",
    "\n",
    "#remove if the file already exists\n",
    "try:\n",
    "    os.remove(dest_file_staypoints)\n",
    "    os.remove(dest_file_hourly_weights)\n",
    "    os.remove(dest_file_final_markov_chain)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "#source paths\n",
    "file_source_raw = \"/home/shashank/Documents/location/Geolife Trajectories 1.3/Data/\" + user + \"/Trajectory/200903*.plt\" \n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------CHANGE HERE FOR USER AND DATE RANGE--------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#read test user trajectory file. In real scenerio, this will be the GPS read data\n",
    "read_usr_file()\n",
    "\n",
    "#prepere dataframes\n",
    "prepare_dfs()\n",
    "\n",
    "#Save first date and time as prev date and time for the start\n",
    "prev_date = usr_trejec_df['Date'][0]\n",
    "prev_hour = usr_trejec_df['Hour'][0]\n",
    "\n",
    "#Read the new locations in an online fashion\n",
    "#  1. Everytime the hour changes, \n",
    "#                  A. Find staypoints for the last hour\n",
    "#                  B. Cluster staypoints based on distance for last hour\n",
    "#                  C. Calculate clusters hourly weights for last hour\n",
    "#                  D. Predict based on trained data\n",
    "#  2. Everytime the date changes,\n",
    "#                  A. Add the days data into training data\n",
    "for i in range(0, len(usr_trejec_df)):\n",
    "    \n",
    "    new_hour = usr_trejec_df['Hour'][i]\n",
    "    new_date = usr_trejec_df['Date'][i]\n",
    "    \n",
    "    if (new_date != prev_date):\n",
    "        if not staypts_df.empty:\n",
    "            k_mean_on_stay_points()\n",
    "            visualize_hourly_cluster_weight()\n",
    "            update_staypts_csv()\n",
    "            update_hourly_weights_csv()\n",
    "            create_save_seperate_trasition_matrices()\n",
    "            create_save_markov_chains()\n",
    "        prev_date = new_date\n",
    "        \n",
    "    if (new_hour != prev_hour) and not curr_hr_df.empty:\n",
    "        \n",
    "        create_last_hr_staypts()\n",
    "        group_clusters()\n",
    "        \n",
    "        if not curr_hr_staypts_df.empty:\n",
    "            cal_hourly_cluster_weight()\n",
    "            read_trained_model()\n",
    "            #predict if training data is available\n",
    "            if not trained_model_df.empty:\n",
    "                predict(new_hour)\n",
    "        prev_hour = new_hour \n",
    "        \n",
    "    else:\n",
    "        curr_hr_df = curr_hr_df.append(usr_trejec_df.iloc[[i]])   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
