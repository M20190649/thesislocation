{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashank/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py:528: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "#Finding stay points from a user trajectory data and calculate transition matrices.\n",
    "#Stay points are goverened by two parameters, time threshold and distance threshold.\n",
    "#If user has spend more than threshold time(10 mins) at the same geographical location(threshold distance 50m),\n",
    "# then this location is users stay point.\n",
    "#This can be users home, work, a bus station or a restaurant. \n",
    "%reset\n",
    "\n",
    "#load all the files for a user\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import math \n",
    "import os\n",
    "import errno\n",
    "import matplotlib.patches as patches\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Ellipse\n",
    "import operator\n",
    "import pdb\n",
    "import calendar\n",
    "#%matplotlib nbagg\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#-----------------------------------LOAD USER FILES-----------------------------------------------------\n",
    "def load_user_file():\n",
    "    global combined_df\n",
    "    #Load file names for user. Change here for different user replace 000 or even path as required\n",
    "    filenames = glob.glob(file_source_raw)\n",
    "\n",
    "    #Read the files\n",
    "    list_of_dfs = [pd.read_csv(filename, skiprows=6, header = None) for filename in filenames]\n",
    "\n",
    "    #put the data from list into one dataframe\n",
    "    combined_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "    #rename columns\n",
    "    combined_df.columns = ['Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "    combined_df['ClusterId'] = -1\n",
    "    combined_df['ClusterMeanLat'] = -1\n",
    "    combined_df['ClusterMeanLon'] = -1\n",
    "    combined_df['StayPoint'] = -1\n",
    "    combined_df['SignificantPlace'] = -1\n",
    "\n",
    "    #add timestamp index\n",
    "    combined_df[\"Timestamp\"] = combined_df[\"Date\"].map(str) + \" \" + combined_df[\"Time\"]\n",
    "    combined_df.Timestamp = pd.to_datetime(combined_df.Timestamp)\n",
    "    combined_df.index = pd.to_datetime(combined_df.Timestamp)\n",
    "    \n",
    "    #add weekday number as column\n",
    "    combined_df['Weekday'] = combined_df['Timestamp'].dt.weekday.map(str) + combined_df['Timestamp'].dt.weekday_name\n",
    "\n",
    "    #combined_df.size\n",
    "    #combined_df.head()\n",
    "\n",
    "#-----------------------------------SAMPLE DATA PER MINUTE-----------------------------------------------------\n",
    "\n",
    "def resample_select_data():\n",
    "    global combined_df\n",
    "    global sampled_df\n",
    "    \n",
    "    #Resample the data with every one minutes. Remove this if you like to process the entire file.\n",
    "    #Note, it could take some time to run the further sections of the file size is very large.\n",
    "    sampled_df = combined_df.resample('1T').mean()\n",
    "    sampled_df[\"Timestamp\"] = sampled_df.index\n",
    "    sampled_df = sampled_df.dropna()\n",
    "    \n",
    "    #Select the range of data you want to proceed with\n",
    "    if fltr_data_date_rng == \"YES\":\n",
    "        sampled_df = sampled_df[(sampled_df[\"Timestamp\"] >= from_date) & (sampled_df[\"Timestamp\"] <= to_date)]\n",
    "\n",
    "#-----------------------------------VISUALIZE RAW DATA--------------------------------------------------------\n",
    "\n",
    "def visualize_raw_data():\n",
    "    \n",
    "    if fltr_data_date_rng == \"YES\":\n",
    "        combined_c_df = combined_df[(combined_df[\"Timestamp\"] >= from_date) & (combined_df[\"Timestamp\"] <= to_date)]\n",
    "    else:\n",
    "        combined_c_df = combined_df\n",
    "        \n",
    "    #Plot 1------------------\n",
    "    #plot hourly vs weekly data of the user\n",
    "    #Resample data on hourly basis\n",
    "    hour_sampled_df = combined_c_df.resample('H').last()\n",
    "\n",
    "    #drop column timestamp to avoid confusion as index is also timestamp\n",
    "    hour_sampled_df = hour_sampled_df.drop(['Timestamp'], axis=1)\n",
    "\n",
    "    #extract time from timestamp and add it as a column\n",
    "    hour_sampled_df['TimeSampled'] = hour_sampled_df.index.time\n",
    "\n",
    "    #form a pivot table counting number of latitudes for each weekday for each hour\n",
    "    pivot_df = hour_sampled_df.pivot_table(values='Latitude', index='Weekday',columns='TimeSampled',aggfunc=len)\n",
    "\n",
    "    #draw a plot to visualize hourly trend per weekday for the user\n",
    "    fig, ax = plt.subplots(figsize=(20,5))  \n",
    "    sns.heatmap(pivot_df, cmap='BuGn', ax=ax)\n",
    "    plt.title('Weekly - Hourly Plot')\n",
    "    \n",
    "    #Plot 2------------------\n",
    "    #visualize counts of data for each month-year\n",
    "    count_df = pd.DataFrame()\n",
    "    count_df['count'] = combined_c_df.groupby(combined_c_df.index.year * 100 + combined_c_df.index.month).size() \n",
    "    count_df.plot(kind='bar', title =\"V comp\",figsize=(5,5),legend=True, fontsize=12, color = 'b')\n",
    "    plt.title('MonthYear Distribution Plot')\n",
    "    \n",
    "    #Plot 3------------------\n",
    "    #plot to visualize the location trend of the user\n",
    "    #Sample data hourly and take mean, drop nans, reset index\n",
    "    xy_df = combined_c_df.resample('H').mean()\n",
    "    xy_df = xy_df.dropna()\n",
    "    xy_df['Timestamp'] = xy_df.index\n",
    "    xy_df = xy_df.reset_index(drop=True)\n",
    "\n",
    "    #assign the first lat and log as the base for the plot i.e. origin\n",
    "    origin_lat = math.radians(xy_df[\"Latitude\"][0])\n",
    "    origin_lon = math.radians(xy_df[\"Longitude\"][0])\n",
    "    \n",
    "    #convert each lat and lon into x and y for the plot w.r.t origin\n",
    "    EARTH_RAD = 6378100\n",
    "    xy_df['X'] = 0.0\n",
    "    xy_df['Y'] = 0.0\n",
    "    for i in range(0, len(xy_df)):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        current_lat = math.radians(xy_df[\"Latitude\"][i])\n",
    "        current_lon = math.radians(xy_df[\"Longitude\"][i])\n",
    "        x = ((math.cos(current_lat) + math.cos(origin_lat))/2) * EARTH_RAD * (current_lon - origin_lon) * math.pi / 180\n",
    "        y = (current_lat - origin_lat)* math.pi/180 * EARTH_RAD\n",
    "        xy_df.at[i, 'X'] = x\n",
    "        xy_df.at[i, 'Y'] = y\n",
    "    \n",
    "    xy_copy_df = xy_df[['X', 'Y', 'Timestamp']].copy()\n",
    "    xy_copy_df['Hour'] = xy_copy_df.Timestamp.dt.hour\n",
    "    del xy_copy_df['Timestamp']\n",
    "    fg = sns.FacetGrid(data=xy_copy_df, hue='Hour',  size = 10, aspect = 2)\n",
    "    fg.map(plt.scatter, 'X', 'Y', edgecolor=\"w\", s=400).add_legend()\n",
    "    plt.title('User Movement Trend Plot')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    xticks = np.arange(-1000,1000,50)\n",
    "    yticks = np.arange(-1000,1000,50)\n",
    "    fg.set(xticks=xticks, yticks=yticks)\n",
    "    plt.savefig(dest_file1)\n",
    "    \n",
    "\n",
    "\n",
    "#-----------------------------------METER DISTANCE BETWEEN TWO COORDINATES---------------------------------------\n",
    "\n",
    "#Find distance between two lan:lon points in meters\n",
    "def meters(lat1, lon1, lat2, lon2):  \n",
    "    R = 6378.137 # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat/2) * math.sin(dLat/2) + math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * math.sin(dLon/2) * math.sin(dLon/2);\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "    d = R * c\n",
    "    return d * 1000 # meters\n",
    "\n",
    "#This function is to cluster the points together if thier distance is less than 50 meters.\n",
    "#If the cluster has a total duration of 10 minutes or greater, then add it to a stay point\n",
    "\n",
    "#-----------------------------------CLUSTER POINTS----------------------------------------------------------\n",
    "\n",
    "def cluster(newlat, newlon, row, count):\n",
    "    global sampled_df\n",
    "    \n",
    "    currcluster = sampled_df['ClusterId'][row-1]\n",
    "    sampled_df['ClusterId'][row] = -1\n",
    "    sampled_df['ClusterMeanLat'][row] = -1\n",
    "    sampled_df['ClusterMeanLon'][row] = -1\n",
    "    sampled_df['StayPoint'][row] = -1\n",
    "    sampled_df['SignificantPlace'][row] = -1\n",
    "    clulat = sampled_df['ClusterMeanLat'][row-1]\n",
    "    clulon = sampled_df['ClusterMeanLon'][row-1]\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    if meters(clulat, clulon, newlat, newlon)<= 100:\n",
    "        sampled_df['ClusterId'][row] = currcluster\n",
    "        sampled_df['ClusterMeanLat'] = sampled_df.groupby('ClusterId')['Latitude'].transform(np.mean)\n",
    "        sampled_df['ClusterMeanLon'] = sampled_df.groupby('ClusterId')['Longitude'].transform(np.mean)\n",
    "        count = count + 1\n",
    "    else:\n",
    "        \n",
    "        if count >= 2:\n",
    "            #import pdb; pdb.set_trace()\n",
    "            MinClusTime = sampled_df['Timestamp'][row-count]\n",
    "            MaxClusTime = sampled_df['Timestamp'][row-1]\n",
    "            k = MaxClusTime - MinClusTime\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            \n",
    "            if (l >= 30):\n",
    "                sampled_df.loc[ (sampled_df['ClusterId']==currcluster), 'StayPoint'] = 1\n",
    "        count = 1\n",
    "        sampled_df['ClusterMeanLat'][row] = sampled_df['Latitude'][row]\n",
    "        sampled_df['ClusterMeanLon'][row] = sampled_df['Longitude'][row]\n",
    "        sampled_df['ClusterId'][row] = currcluster + 1\n",
    "    return count\n",
    "\n",
    "#-----------------------------------FIND STAYPOINTS USING CLUSTERS---------------------------------------------------\n",
    "\n",
    "def find_stay_points():\n",
    "    global sampled_df \n",
    "    global staypts_df\n",
    "    \n",
    "    #Read the file in an online manner as the points come and assign the points to clusters\n",
    "    row =1\n",
    "    count = 1\n",
    "    sampled_df['ClusterId'][row-1] = 0\n",
    "    sampled_df['ClusterMeanLat'][row-1] = sampled_df['Latitude'][0]\n",
    "    sampled_df['ClusterMeanLon'][row-1] = sampled_df['Longitude'][0]\n",
    "    sampled_df['StayPoint'][row-1] = -1\n",
    "    sampled_df['SignificantPlace'][row-1] = -1\n",
    "    while row < len(sampled_df):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        count = cluster(sampled_df['Latitude'][row], sampled_df['Longitude'][row], row, count)\n",
    "        row= row + 1\n",
    "\n",
    "    #copy the stay points into another dataframe\n",
    "    staypts_df = sampled_df.loc[sampled_df['StayPoint'] == 1]\n",
    "    \n",
    "#-----------------------------------GROUP CLUSTERS ON DIFFERENT DAYS-----------------------------------------------\n",
    "\n",
    "def group_clusters():\n",
    "    global staypts_df\n",
    "    \n",
    "    #this fucntion groups the clusters together from different days \n",
    "    #Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    staypts_df1 = staypts_df[['ClusterId', 'ClusterMeanLat', 'ClusterMeanLon']].copy()\n",
    "    staypts_df1 = staypts_df1.drop_duplicates(subset=['ClusterId', 'ClusterMeanLat', 'ClusterMeanLon'])\n",
    "\n",
    "    staypts_df1 = staypts_df1.sort_values(['ClusterMeanLat', 'ClusterMeanLon'])\n",
    "\n",
    "    row = 1\n",
    "    for i in range(0, len(staypts_df1)):\n",
    "        for j in range(i+1, len(staypts_df1)):\n",
    "            #import pdb; pdb.set_trace()\n",
    "        \n",
    "            chk_cluster = staypts_df1['ClusterId'][i]\n",
    "            chk_clulat = staypts_df1['ClusterMeanLat'][i]\n",
    "            chk_clulon = staypts_df1['ClusterMeanLon'][i]\n",
    "            curr_cluster = staypts_df1['ClusterId'][j]\n",
    "            curr_clulat = staypts_df1['ClusterMeanLat'][j]\n",
    "            curr_clulon = staypts_df1['ClusterMeanLon'][j]\n",
    "        \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= 100:\n",
    "                staypts_df.loc[ (staypts_df['ClusterId']==curr_cluster), 'ClusterId'] = chk_cluster\n",
    "                staypts_df['ClusterMeanLat'] = staypts_df.groupby('ClusterId')['ClusterMeanLat'].transform(np.mean)\n",
    "                staypts_df['ClusterMeanLon'] = staypts_df.groupby('ClusterId')['ClusterMeanLon'].transform(np.mean)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "#-----------------------------------SAVE STAYPOINTS AS FILE-----------------------------------------------------\n",
    "\n",
    "def save_file_stay_points():\n",
    "    #Save the file to a location for further analysis\n",
    "    staypts_df.to_csv(dest_file_staypoints, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#------------------------------------VISUALIZE STAYPOINTS-------------------------------------------------------\n",
    "\n",
    "def visualize_stay_points():\n",
    "    global staypts_df\n",
    "    #Plot the significant places\n",
    "\n",
    "#     #load the staypoints file\n",
    "#     staypts_df = pd.read_csv(dest_file_staypoints, sep='\\t')\n",
    "\n",
    "    #make index as datetime\n",
    "    staypts_df.index = pd.to_datetime(staypts_df.Timestamp)\n",
    "\n",
    "    #assign the first lat and log as the base for the plot i.e. origin\n",
    "    origin_lat = math.radians(combined_df[\"Latitude\"][0])\n",
    "    origin_lon = math.radians(combined_df[\"Longitude\"][0])\n",
    "    staypts_df['Timestamp'] = staypts_df.index\n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "\n",
    "    #convert each lat and lon into x and y for the plot w.r.t origin\n",
    "    EARTH_RAD = 6378100\n",
    "    staypts_df['X'] = 0.0\n",
    "    staypts_df['Y'] = 0.0\n",
    "    for i in range(0, len(staypts_df)):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        current_lat = math.radians(staypts_df[\"Latitude\"][i])\n",
    "        current_lon = math.radians(staypts_df[\"Longitude\"][i])\n",
    "        x = ((math.cos(current_lat) + math.cos(origin_lat))/2) * EARTH_RAD * (current_lon - origin_lon) * math.pi / 180\n",
    "        y = (current_lat - origin_lat)* math.pi/180 * EARTH_RAD\n",
    "        staypts_df.at[i, 'X'] = x\n",
    "        staypts_df.at[i, 'Y'] = y\n",
    "\n",
    "    #plot the x and y's\n",
    "    staypts_copy_df = staypts_df[['X', 'Y', 'Timestamp']].copy()\n",
    "    staypts_copy_df['Hour'] = staypts_copy_df.Timestamp.dt.hour\n",
    "    del staypts_copy_df['Timestamp']\n",
    "    #xy_test_df.head()\n",
    "    fg = sns.FacetGrid(data=staypts_copy_df, hue='Hour',   size = 10, aspect = 2)\n",
    "    fg.map(plt.scatter, 'X', 'Y', edgecolor=\"w\", s = 400).add_legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title(\"User Staypoints Trend Plot\")\n",
    "    xticks = np.arange(-1000,1000,50)\n",
    "    yticks = np.arange(-1000,1000,50)\n",
    "    fg.set(xticks=xticks, yticks=yticks)\n",
    "    plt.savefig(dest_file2)\n",
    "    \n",
    "    staypts_df.index = pd.to_datetime(staypts_df.Timestamp)\n",
    "#-----------------------------------CALCULATE HOURLY WEIGHTS FOR STAYPOINTS--------------------------------------\n",
    "def cal_hourly_cluster_weight():\n",
    "    global staypts_df\n",
    "    global cluster_hourly_df    \n",
    "\n",
    "    for i in range(0, 24):\n",
    "        cluster_hourly_df['Date'] = 0\n",
    "        cluster_hourly_df['ClusterId'] = 0\n",
    "        cluster_hourly_df['AvgLat'] = 0\n",
    "        cluster_hourly_df['AvgLon'] = 0\n",
    "        cluster_hourly_df[i] = 0\n",
    "\n",
    "    last_hour = staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = staypts_df['ClusterId'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(0, len(staypts_df)):\n",
    "\n",
    "        if (i == len(staypts_df)-1):\n",
    "            \n",
    "            k = staypts_df['Timestamp'][i] - staypts_df['Timestamp'][i-curr_count+1]\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            \n",
    "            date_read = staypts_df.index[i].date()\n",
    "            cluster_id = staypts_df['ClusterId'][i]\n",
    "            ClusterMeanLat = staypts_df['ClusterMeanLat'][i]\n",
    "            ClusterMeanLon = staypts_df['ClusterMeanLon'][i]\n",
    "            col_name = staypts_df.index[i].hour\n",
    "\n",
    "            cluster_hourly_df.loc[j,'AvgLat'] = ClusterMeanLat\n",
    "            cluster_hourly_df.loc[j,'AvgLon'] = ClusterMeanLon\n",
    "            cluster_hourly_df.loc[j,'Date'] = date_read\n",
    "            cluster_hourly_df.loc[j,'ClusterId'] = cluster_id\n",
    "            cluster_hourly_df.loc[j, col_name] = round((l)/60,4)\n",
    "            \n",
    "        if (staypts_df['Timestamp'][i].hour != last_hour) | (staypts_df['ClusterId'][i] != last_clusid):\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "            if (curr_count == 1) & (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                k = ((staypts_df['Timestamp'][i-1] + pd.Timedelta(hours=1) - \n",
    "                      pd.Timedelta(minutes=staypts_df['Timestamp'][i-1].minute)) - \n",
    "                     staypts_df['Timestamp'][i-1])\n",
    "            else:\n",
    "                k = staypts_df['Timestamp'][i-1] - staypts_df['Timestamp'][i-curr_count]\n",
    "\n",
    "            l = (k / np.timedelta64(1, 'm')).astype(int)\n",
    "            date_read = staypts_df.index[i-1].date()\n",
    "            cluster_id = staypts_df['ClusterId'][i-1]\n",
    "            ClusterMeanLat = staypts_df['ClusterMeanLat'][i-1]\n",
    "            ClusterMeanLon = staypts_df['ClusterMeanLon'][i-1]\n",
    "            col_name = staypts_df.index[i-1].hour\n",
    "\n",
    "            cluster_hourly_df.loc[j, 'AvgLat'] = ClusterMeanLat\n",
    "            cluster_hourly_df.loc[j, 'AvgLon'] = ClusterMeanLon\n",
    "            cluster_hourly_df.loc[j, 'Date'] = date_read\n",
    "            cluster_hourly_df.loc[j, 'ClusterId'] = cluster_id\n",
    "            cluster_hourly_df.loc[j, col_name] = round((l)/60,4)\n",
    "            j = j + 1\n",
    "            curr_count = 1\n",
    "\n",
    "            if (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                last_hour = staypts_df['Timestamp'][i].hour\n",
    "            if (staypts_df['ClusterId'][i] != last_clusid):\n",
    "                last_clusid = staypts_df['ClusterId'][i]\n",
    "        else:\n",
    "            curr_count = curr_count + 1\n",
    "\n",
    "    cluster_hourly_df = cluster_hourly_df.fillna(0)\n",
    "    cluster_hourly_df = cluster_hourly_df.groupby(['Date', 'ClusterId', 'AvgLat', 'AvgLon']).sum()\n",
    "    cluster_hourly_df = cluster_hourly_df.reset_index(level=[0,1,2,3])\n",
    "\n",
    "#--------------------------------VISUALIZE HOURLY CLUSTER WEIGHT-------------------------------------------------\n",
    "def visualize_hourly_cluster_weight():\n",
    "    global staypts_df\n",
    "    \n",
    "    #create a color dictionary for each cluster for the plot\n",
    "    dicts = {}\n",
    "    clu_list = []\n",
    "    clu_list = staypts_df['ClusterId'].unique()\n",
    "    #r = lambda: random.randint(0,255)\n",
    "    colors = sns.color_palette(\"Paired\", len(clu_list))\n",
    "    \n",
    "    for i in range(0, len(clu_list)):\n",
    "        dicts[clu_list[i]] = (colors[i])\n",
    "        #dicts[clu_list[i]] = ('#%02X%02X%02X' % (r(),r(),r()))\n",
    "        \n",
    "    #create a new graph where we will later add rectangles for each hour:cluster\n",
    "    fig2 = plt.figure(figsize=(15,15))\n",
    "    ax1 = fig2.add_subplot(111, aspect='equal')\n",
    "\n",
    "    #get all the dates for y axis\n",
    "    date_list = staypts_df['Timestamp'].dt.date.unique()\n",
    "    y = range(0, len(date_list))\n",
    "    def_yticks = date_list\n",
    "    plt.yticks(y, def_yticks)\n",
    "    \n",
    "    #set the x axis limit from 0-24 hours of a day, y axis with dates\n",
    "    limsx = (0, 24)\n",
    "    limsy = (0, len(date_list))\n",
    "\n",
    "    date_counter = 0\n",
    "    last_date = staypts_df['Timestamp'][0].date()\n",
    "    last_hour = staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = staypts_df['ClusterId'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        ax1.axvline(x= i, linewidth=1, color='r')\n",
    "\n",
    "    for i in range(0, len(staypts_df)):\n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        #plot a rectangle if the hour or clusterid or date has changed\n",
    "        if ((staypts_df['Timestamp'][i].hour != last_hour) | (staypts_df['ClusterId'][i] != last_clusid)\n",
    "           | (last_date != staypts_df['Timestamp'][i].date())):\n",
    "\n",
    "            if (curr_count == 1) & (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                a = staypts_df['Timestamp'][i-curr_count].hour + 1\n",
    "            else:\n",
    "                a = staypts_df['Timestamp'][i-curr_count].hour + staypts_df['Timestamp'][i-curr_count].minute/60\n",
    "\n",
    "            b = staypts_df['Timestamp'][i-1].hour + staypts_df['Timestamp'][i-1].minute/60\n",
    "\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            col_id = dicts.get(staypts_df['ClusterId'][i-1])\n",
    "            ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=staypts_df['ClusterId'][i-1]))\n",
    "\n",
    "            curr_count = 1\n",
    "\n",
    "            if (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                last_hour = staypts_df['Timestamp'][i].hour\n",
    "            if (staypts_df['ClusterId'][i] != last_clusid):\n",
    "                last_clusid = staypts_df['ClusterId'][i]\n",
    "            if (last_date != staypts_df['Timestamp'][i].date()):\n",
    "                date_counter = date_counter + 1\n",
    "                last_date = staypts_df['Timestamp'][i].date()\n",
    "                ax1.axhline(y= date_counter, linewidth=1, color='r')\n",
    "\n",
    "        else:\n",
    "            curr_count = curr_count + 1\n",
    "            \n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handle_list, label_list = [], []\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if label not in label_list:\n",
    "            handle_list.append(handle)\n",
    "            label_list.append(label)\n",
    "    plt.legend(handle_list, label_list)\n",
    "\n",
    "    plt.xlim(limsx)\n",
    "    plt.ylim(limsy)\n",
    "    plt.savefig(dest_file3)\n",
    "\n",
    "\n",
    "#---------------------------------------------- S  T  A  R  T  -------------------------------------------------\n",
    "#Start of the program\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "\n",
    "\n",
    "#Load file names for users\n",
    "for i in range(0, 51):\n",
    "    #global dataframes used\n",
    "    combined_df = pd.DataFrame()\n",
    "    sampled_df = pd.DataFrame()\n",
    "    staypts_df = pd.DataFrame()\n",
    "    cluster_hourly_df = pd.DataFrame()\n",
    "\n",
    "    user = \"%03d\"%i\n",
    "    path = \"/home/shashank/Documents/location/Geolife Trajectories 1.3/Data/\" + str(user) + \"/Trajectory/20*.plt\"\n",
    "    \n",
    "    filenames = glob.glob(path)\n",
    "\n",
    "    #Read the files\n",
    "    list_of_dfs = [pd.read_csv(filename, skiprows=6, header = None) for filename in filenames]\n",
    "\n",
    "    #put the data from list into one dataframe\n",
    "    combined_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "    #rename columns\n",
    "    combined_df.columns = ['Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "\n",
    "    #add filename as column to identify each row with its source\n",
    "    for filename in filenames:\n",
    "        combined_df['Filename'] = filename[-18:]\n",
    "\n",
    "    #add timestamp as columns and then make timestamp as index\n",
    "    combined_df[\"Timestamp\"] = combined_df[\"Date\"].map(str) + \" \" + combined_df[\"Time\"]\n",
    "    combined_df.Timestamp = pd.to_datetime(combined_df.Timestamp)\n",
    "    combined_df.index = pd.to_datetime(combined_df.Timestamp)\n",
    "    \n",
    "    test = pd.DataFrame()\n",
    "    test['count'] = combined_df.groupby(combined_df.index.year * 100 + combined_df.index.month).size() \n",
    "    test['MonthYear'] = test.index\n",
    "    mnth_yr = test['MonthYear'].loc[test['count'] == max(test['count'])]\n",
    "    mnth_yr = mnth_yr.iloc[0]\n",
    "    mnth_yr = str(mnth_yr)\n",
    "    yr = mnth_yr[:4]\n",
    "    mnth = mnth_yr[4:]\n",
    "    mnth = int(mnth)\n",
    "    yr = int(yr)\n",
    "    end_date = calendar.monthrange(yr, mnth)[1]\n",
    "\n",
    "    #source, destination path and filter till date(till when the data needs to be considered). Change according to your needs\n",
    "    from_date = str(yr) + \"-\" + str(mnth) + \"-01 00:00:00\"\n",
    "    to_date = str(yr) + \"-\" + str(mnth) + \"-\" + str(end_date) + \" 23:59:00\"\n",
    "    fltr_data_date_rng = \"YES\"   \n",
    "\n",
    "    \n",
    "    file_source_raw = \"/home/shashank/Documents/location/Geolife Trajectories 1.3/Data/\" + str(user) + \"/Trajectory/20*.plt\" \n",
    "    destination = \"/home/shashank/Documents/location/Data/User stay points(month-maxdata, 100m 30min)/\"\n",
    "    dest_file1 = destination + str(user) + \" YearMonth-\" + str(yr) + str(mnth) + \" Raw Data.png\" \n",
    "    dest_file2 = destination + str(user) + \" YearMonth-\" + str(yr) + str(mnth) + \" Staypoints.png\" \n",
    "    dest_file3 = destination + str(user) + \" YearMonth-\" + str(yr) + str(mnth) + \" Hourly Cluster Weights.png\" \n",
    "\n",
    "    load_user_file()\n",
    "\n",
    "    resample_select_data()\n",
    "\n",
    "    visualize_raw_data()\n",
    "\n",
    "    find_stay_points()\n",
    "    if not staypts_df.empty:\n",
    "        group_clusters()\n",
    "\n",
    "        visualize_stay_points()\n",
    "    \n",
    "        cal_hourly_cluster_weight()\n",
    "\n",
    "        visualize_hourly_cluster_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
